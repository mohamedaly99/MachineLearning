{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Programming Exercise 2: Logistic Regression\n\n## Introduction\n\nIn this exercise, you will implement logistic regression and apply it to two different datasets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# used for manipulating directory paths\nimport os\n\n# Scientific and vector computation for python\nimport numpy as np\n\n# Plotting library\nfrom matplotlib import pyplot\n\n# Optimization module in scipy\nfrom scipy import optimize\n\n# library written for this exercise providing additional functions for assignment submission, and others\nimport utils\n\n# tells matplotlib to embed plots within the notebook\n%matplotlib inline","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 Logistic Regression\n\nIn this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and\nyou want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions\ndecision. Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams. \n\nThe following cell will load the data and corresponding labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.loadtxt(os.path.join('ex2data1.txt'), delimiter=',')\n#this is used to shuffle the data randomize the output\nnp.random.shuffle(data)\n\nX, y = data[:, 0:2], data[:, 2]\n#adding the bais collumn of ones\nX = np.concatenate([np.ones((data.shape[0], 1)), X], axis=1)\nprint(X)","execution_count":11,"outputs":[{"output_type":"stream","text":"[[ 1.         85.40451939 57.05198398]\n [ 1.         34.62365962 78.02469282]\n [ 1.         51.54772027 46.85629026]\n [ 1.         62.22267576 52.06099195]\n [ 1.         34.21206098 44.2095286 ]\n [ 1.         82.36875376 40.61825516]\n [ 1.         60.45788574 73.0949981 ]\n [ 1.         99.8278578  72.36925193]\n [ 1.         75.02474557 46.55401354]\n [ 1.         94.44336777 65.56892161]\n [ 1.         68.46852179 85.5943071 ]\n [ 1.         67.37202755 42.83843832]\n [ 1.         77.92409145 68.97235999]\n [ 1.         30.05882245 49.59297387]\n [ 1.         74.78925296 41.57341523]\n [ 1.         82.30705337 76.4819633 ]\n [ 1.         42.07545454 78.844786  ]\n [ 1.         78.63542435 96.64742717]\n [ 1.         79.03273605 75.34437644]\n [ 1.         80.19018075 44.82162893]\n [ 1.         77.19303493 70.4582    ]\n [ 1.         50.4581598  75.80985953]\n [ 1.         70.66150955 92.92713789]\n [ 1.         94.09433113 77.15910509]\n [ 1.         72.34649423 96.22759297]\n [ 1.         32.72283304 43.30717306]\n [ 1.         58.84095622 75.85844831]\n [ 1.         83.48916274 48.3802858 ]\n [ 1.         66.74671857 60.99139403]\n [ 1.         51.04775177 45.82270146]\n [ 1.         67.31925747 66.58935318]\n [ 1.         61.83020602 50.25610789]\n [ 1.         94.83450672 45.6943068 ]\n [ 1.         54.63510555 52.21388588]\n [ 1.         56.2538175  39.26147251]\n [ 1.         99.31500881 68.77540947]\n [ 1.         84.43281996 43.53339331]\n [ 1.         50.53478829 48.85581153]\n [ 1.         50.28649612 49.80453881]\n [ 1.         76.97878373 47.57596365]\n [ 1.         75.01365839 30.60326323]\n [ 1.         75.47770201 90.424539  ]\n [ 1.         97.64563396 68.86157272]\n [ 1.         35.84740877 72.90219803]\n [ 1.         79.94481794 74.16311935]\n [ 1.         40.45755098 97.53518549]\n [ 1.         44.66826172 66.45008615]\n [ 1.         66.56089447 41.09209808]\n [ 1.         90.54671411 43.39060181]\n [ 1.         89.84580671 45.35828361]\n [ 1.         45.08327748 56.31637178]\n [ 1.         55.34001756 64.93193801]\n [ 1.         38.7858038  64.99568096]\n [ 1.         90.44855097 87.50879176]\n [ 1.         62.0730638  96.76882412]\n [ 1.         60.18259939 86.3085521 ]\n [ 1.         55.48216114 35.57070347]\n [ 1.         76.0987867  87.42056972]\n [ 1.         33.91550011 98.86943574]\n [ 1.         89.67677575 65.79936593]\n [ 1.         49.58667722 59.80895099]\n [ 1.         71.79646206 78.45356225]\n [ 1.         39.53833914 76.03681085]\n [ 1.         82.22666158 42.71987854]\n [ 1.         67.94685548 46.67857411]\n [ 1.         80.366756   90.9601479 ]\n [ 1.         61.10666454 96.51142588]\n [ 1.         99.27252693 60.999031  ]\n [ 1.         74.24869137 69.82457123]\n [ 1.         34.52451385 60.39634246]\n [ 1.         88.91389642 69.8037889 ]\n [ 1.         49.07256322 51.88321182]\n [ 1.         69.07014406 52.74046973]\n [ 1.         52.34800399 60.76950526]\n [ 1.         74.775893   89.5298129 ]\n [ 1.         61.37928945 72.80788731]\n [ 1.         35.28611282 47.02051395]\n [ 1.         75.39561147 85.75993667]\n [ 1.         52.04540477 69.43286012]\n [ 1.         64.03932042 78.03168802]\n [ 1.         34.18364003 75.23772034]\n [ 1.         74.49269242 84.84513685]\n [ 1.         91.5649745  88.69629255]\n [ 1.         32.57720017 95.59854761]\n [ 1.         62.27101367 69.95445795]\n [ 1.         97.77159928 86.72782233]\n [ 1.         52.10797973 63.12762377]\n [ 1.         42.26170081 87.10385094]\n [ 1.         83.90239366 56.30804622]\n [ 1.         53.97105215 89.20735014]\n [ 1.         47.26426911 88.475865  ]\n [ 1.         69.36458876 97.71869196]\n [ 1.         40.23689374 71.16774802]\n [ 1.         64.17698887 80.90806059]\n [ 1.         95.86155507 38.22527806]\n [ 1.         93.1143888  38.80067034]\n [ 1.         60.45555629 42.50840944]\n [ 1.         80.27957401 92.11606081]\n [ 1.         57.23870632 59.51428198]\n [ 1.         30.28671077 43.89499752]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_X = np.zeros((X.shape[0],4))\n\nnew_X[:,0:3] = X[:,0:3]\n\nnew_X[:,3] = X[:,1]**2\nprint(new_X)\n\n#deviding the set size into train=60 cross=20 test=20\nX_train = new_X[0:60,:]\ny_train = y[0:60]\nX_cross = new_X[60:80,:]\ny_cross = y[60:80]\nX_test = new_X[80:100,:]\ny_test = y[80:100]","execution_count":12,"outputs":[{"output_type":"stream","text":"[[1.00000000e+00 8.54045194e+01 5.70519840e+01 7.29393193e+03]\n [1.00000000e+00 3.46236596e+01 7.80246928e+01 1.19879781e+03]\n [1.00000000e+00 5.15477203e+01 4.68562903e+01 2.65716746e+03]\n [1.00000000e+00 6.22226758e+01 5.20609919e+01 3.87166138e+03]\n [1.00000000e+00 3.42120610e+01 4.42095286e+01 1.17046512e+03]\n [1.00000000e+00 8.23687538e+01 4.06182552e+01 6.78461160e+03]\n [1.00000000e+00 6.04578857e+01 7.30949981e+01 3.65515595e+03]\n [1.00000000e+00 9.98278578e+01 7.23692519e+01 9.96560119e+03]\n [1.00000000e+00 7.50247456e+01 4.65540135e+01 5.62871245e+03]\n [1.00000000e+00 9.44433678e+01 6.55689216e+01 8.91954972e+03]\n [1.00000000e+00 6.84685218e+01 8.55943071e+01 4.68793848e+03]\n [1.00000000e+00 6.73720275e+01 4.28384383e+01 4.53899010e+03]\n [1.00000000e+00 7.79240915e+01 6.89723600e+01 6.07216403e+03]\n [1.00000000e+00 3.00588224e+01 4.95929739e+01 9.03532807e+02]\n [1.00000000e+00 7.47892530e+01 4.15734152e+01 5.59343236e+03]\n [1.00000000e+00 8.23070534e+01 7.64819633e+01 6.77445104e+03]\n [1.00000000e+00 4.20754545e+01 7.88447860e+01 1.77034387e+03]\n [1.00000000e+00 7.86354243e+01 9.66474272e+01 6.18352996e+03]\n [1.00000000e+00 7.90327361e+01 7.53443764e+01 6.24617337e+03]\n [1.00000000e+00 8.01901808e+01 4.48216289e+01 6.43046509e+03]\n [1.00000000e+00 7.71930349e+01 7.04582000e+01 5.95876464e+03]\n [1.00000000e+00 5.04581598e+01 7.58098595e+01 2.54602589e+03]\n [1.00000000e+00 7.06615096e+01 9.29271379e+01 4.99304893e+03]\n [1.00000000e+00 9.40943311e+01 7.71591051e+01 8.85374315e+03]\n [1.00000000e+00 7.23464942e+01 9.62275930e+01 5.23401523e+03]\n [1.00000000e+00 3.27228330e+01 4.33071731e+01 1.07078380e+03]\n [1.00000000e+00 5.88409562e+01 7.58584483e+01 3.46225813e+03]\n [1.00000000e+00 8.34891627e+01 4.83802858e+01 6.97044030e+03]\n [1.00000000e+00 6.67467186e+01 6.09913940e+01 4.45512444e+03]\n [1.00000000e+00 5.10477518e+01 4.58227015e+01 2.60587296e+03]\n [1.00000000e+00 6.73192575e+01 6.65893532e+01 4.53188243e+03]\n [1.00000000e+00 6.18302060e+01 5.02561079e+01 3.82297438e+03]\n [1.00000000e+00 9.48345067e+01 4.56943068e+01 8.99358367e+03]\n [1.00000000e+00 5.46351056e+01 5.22138859e+01 2.98499476e+03]\n [1.00000000e+00 5.62538175e+01 3.92614725e+01 3.16449198e+03]\n [1.00000000e+00 9.93150088e+01 6.87754095e+01 9.86347097e+03]\n [1.00000000e+00 8.44328200e+01 4.35333933e+01 7.12890109e+03]\n [1.00000000e+00 5.05347883e+01 4.88558115e+01 2.55376483e+03]\n [1.00000000e+00 5.02864961e+01 4.98045388e+01 2.52873169e+03]\n [1.00000000e+00 7.69787837e+01 4.75759636e+01 5.92573314e+03]\n [1.00000000e+00 7.50136584e+01 3.06032632e+01 5.62704894e+03]\n [1.00000000e+00 7.54777020e+01 9.04245390e+01 5.69688350e+03]\n [1.00000000e+00 9.76456340e+01 6.88615727e+01 9.53466983e+03]\n [1.00000000e+00 3.58474088e+01 7.29021980e+01 1.28503672e+03]\n [1.00000000e+00 7.99448179e+01 7.41631194e+01 6.39117392e+03]\n [1.00000000e+00 4.04575510e+01 9.75351855e+01 1.63681343e+03]\n [1.00000000e+00 4.46682617e+01 6.64500861e+01 1.99525361e+03]\n [1.00000000e+00 6.65608945e+01 4.10920981e+01 4.43035267e+03]\n [1.00000000e+00 9.05467141e+01 4.33906018e+01 8.19870744e+03]\n [1.00000000e+00 8.98458067e+01 4.53582836e+01 8.07226898e+03]\n [1.00000000e+00 4.50832775e+01 5.63163718e+01 2.03250191e+03]\n [1.00000000e+00 5.53400176e+01 6.49319380e+01 3.06251754e+03]\n [1.00000000e+00 3.87858038e+01 6.49956810e+01 1.50433858e+03]\n [1.00000000e+00 9.04485510e+01 8.75087918e+01 8.18094037e+03]\n [1.00000000e+00 6.20730638e+01 9.67688241e+01 3.85306525e+03]\n [1.00000000e+00 6.01825994e+01 8.63085521e+01 3.62194527e+03]\n [1.00000000e+00 5.54821611e+01 3.55707035e+01 3.07827020e+03]\n [1.00000000e+00 7.60987867e+01 8.74205697e+01 5.79102534e+03]\n [1.00000000e+00 3.39155001e+01 9.88694357e+01 1.15026115e+03]\n [1.00000000e+00 8.96767758e+01 6.57993659e+01 8.04192411e+03]\n [1.00000000e+00 4.95866772e+01 5.98089510e+01 2.45883856e+03]\n [1.00000000e+00 7.17964621e+01 7.84535622e+01 5.15473196e+03]\n [1.00000000e+00 3.95383391e+01 7.60368109e+01 1.56328026e+03]\n [1.00000000e+00 8.22266616e+01 4.27198785e+01 6.76122387e+03]\n [1.00000000e+00 6.79468555e+01 4.66785741e+01 4.61677517e+03]\n [1.00000000e+00 8.03667560e+01 9.09601479e+01 6.45881547e+03]\n [1.00000000e+00 6.11066645e+01 9.65114259e+01 3.73402445e+03]\n [1.00000000e+00 9.92725269e+01 6.09990310e+01 9.85503460e+03]\n [1.00000000e+00 7.42486914e+01 6.98245712e+01 5.51286817e+03]\n [1.00000000e+00 3.45245139e+01 6.03963425e+01 1.19194206e+03]\n [1.00000000e+00 8.89138964e+01 6.98037889e+01 7.90568098e+03]\n [1.00000000e+00 4.90725632e+01 5.18832118e+01 2.40811646e+03]\n [1.00000000e+00 6.90701441e+01 5.27404697e+01 4.77068480e+03]\n [1.00000000e+00 5.23480040e+01 6.07695053e+01 2.74031352e+03]\n [1.00000000e+00 7.47758930e+01 8.95298129e+01 5.59143417e+03]\n [1.00000000e+00 6.13792894e+01 7.28078873e+01 3.76741717e+03]\n [1.00000000e+00 3.52861128e+01 4.70205139e+01 1.24510976e+03]\n [1.00000000e+00 7.53956115e+01 8.57599367e+01 5.68449823e+03]\n [1.00000000e+00 5.20454048e+01 6.94328601e+01 2.70872416e+03]\n [1.00000000e+00 6.40393204e+01 7.80316880e+01 4.10103456e+03]\n [1.00000000e+00 3.41836400e+01 7.52377203e+01 1.16852125e+03]\n [1.00000000e+00 7.44926924e+01 8.48451368e+01 5.54916122e+03]\n [1.00000000e+00 9.15649745e+01 8.86962925e+01 8.38414455e+03]\n [1.00000000e+00 3.25772002e+01 9.55985476e+01 1.06127397e+03]\n [1.00000000e+00 6.22710137e+01 6.99544580e+01 3.87767914e+03]\n [1.00000000e+00 9.77715993e+01 8.67278223e+01 9.55928563e+03]\n [1.00000000e+00 5.21079797e+01 6.31276238e+01 2.71524155e+03]\n [1.00000000e+00 4.22617008e+01 8.71038509e+01 1.78605136e+03]\n [1.00000000e+00 8.39023937e+01 5.63080462e+01 7.03961166e+03]\n [1.00000000e+00 5.39710521e+01 8.92073501e+01 2.91287447e+03]\n [1.00000000e+00 4.72642691e+01 8.84758650e+01 2.23391113e+03]\n [1.00000000e+00 6.93645888e+01 9.77186920e+01 4.81144617e+03]\n [1.00000000e+00 4.02368937e+01 7.11677480e+01 1.61900762e+03]\n [1.00000000e+00 6.41769889e+01 8.09080606e+01 4.11868590e+03]\n [1.00000000e+00 9.58615551e+01 3.82252781e+01 9.18943774e+03]\n [1.00000000e+00 9.31143888e+01 3.88006703e+01 8.67028940e+03]\n [1.00000000e+00 6.04555563e+01 4.25084094e+01 3.65487429e+03]\n [1.00000000e+00 8.02795740e+01 9.21160608e+01 6.44481000e+03]\n [1.00000000e+00 5.72387063e+01 5.95142820e+01 3.27626950e+03]\n [1.00000000e+00 3.02867108e+01 4.38949975e+01 9.17284849e+02]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"newNew_X = np.zeros((X.shape[0],4))\nnewNew_X[:,0:3] = X[: ,0:3]\nnewNew_X[:,2] = X[:, 2]**2\nX_Newtrain = newNew_X[0:60,:]\ny_Newtrain = y[0:60]\nX_Newcross = newNew_X[60:80,:]\ny_Newcross = y[60:80]\nX_Newtest = newNew_X[80:100,:]\ny_Newtest = y[80:100]","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    \"\"\"\n    Compute sigmoid function given the input z.\n    \n    Parameters\n    ----------\n    z : array_like\n        The input to the sigmoid function. This can be a 1-D vector \n        or a 2-D matrix. \n    \n    Returns\n    -------\n    g : array_like\n        The computed sigmoid function. g has the same shape as z, since\n        the sigmoid is computed element-wise on z.\n        \n    Instructions\n    ------------\n    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n    \"\"\"\n    # convert input to a numpy array\n    z = np.array(z)\n    \n    # You need to return the following variables correctly \n    g = np.zeros(z.shape)\n\n    # ====================== YOUR CODE HERE ======================\n    g = 1 / (1 + np.exp(-z))\n\n    # =============================================================\n    return g","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def costFunction(theta, X, y):\n    \"\"\"\n    Compute cost and gradient for logistic regression. \n    \n    Parameters\n    ----------\n    theta : array_like\n        The parameters for logistic regression. This a vector\n        of shape (n+1, ).\n    \n    X : array_like\n        The input dataset of shape (m x n+1) where m is the total number\n        of data points and n is the number of features. We assume the \n        intercept has already been added to the input.\n    \n    y : arra_like\n        Labels for the input. This is a vector of shape (m, ).\n    \n    Returns\n    -------\n    J : float\n        The computed value for the cost function. \n    \n    grad : array_like\n        A vector of shape (n+1, ) which is the gradient of the cost\n        function with respect to theta, at the current values of theta.\n        \n    Instructions\n    ------------\n    Compute the cost of a particular choice of theta. You should set J to \n    the cost. Compute the partial derivatives and set grad to the partial\n    derivatives of the cost w.r.t. each parameter in theta.\n    \"\"\"\n    # Initialize some useful values\n    m = y.size  # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n    grad = np.zeros(theta.shape)\n\n    # ====================== YOUR CODE HERE ======================\n    h = sigmoid(X.dot(theta.T))\n    \n    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n    grad = (1 / m) * (h - y).dot(X)\n    \n    \n    # =============================================================\n    return J, grad","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeCost(theta,X,y):\n    # Initialize some useful values\n    m = y.size  # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n\n    # ====================== YOUR CODE HERE ======================\n    h = sigmoid(X.dot(theta.T))\n    \n    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n    \n    return J","execution_count":42,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once you are done call your `costFunction` using two test cases for  $\\theta$ by executing the next cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"options= {'maxiter': 400}\ninitial_theta = np.zeros(3)\nres = optimize.minimize(costFunction,initial_theta,(X_train[:,0:3], y_train),jac=True,method='TNC',options=options)\nthata1 = res.x\nprint ( res )","execution_count":43,"outputs":[{"output_type":"stream","text":"     fun: 0.1142909479044377\n     jac: array([ 8.96064706e-07, -8.20471970e-07,  2.49710283e-03])\n message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n    nfev: 52\n     nit: 22\n  status: 1\n success: True\n       x: array([-41.60836284,   0.37752445,   0.30006989])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"options= {'maxiter': 400}\ninitial_theta = np.zeros(3)\nres = optimize.minimize(costFunction,initial_theta,(X_Newtrain[:,0:3], y_Newtrain),jac=True,method='TNC',options=options)\nthata2 = res.x\nprint ( res )","execution_count":49,"outputs":[{"output_type":"stream","text":"     fun: 0.15947446031345885\n     jac: array([0.00041525, 0.02064597, 0.00430398])\n message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n    nfev: 64\n     nit: 28\n  status: 1\n success: True\n       x: array([-2.27812985e+01,  2.67717690e-01,  1.54839025e-03])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(theta, X):\n    \"\"\"\n    Predict whether the label is 0 or 1 using learned logistic regression.\n    Computes the predictions for X using a threshold at 0.5 \n    (i.e., if sigmoid(theta.T*x) >= 0.5, predict 1)\n    \n    Parameters\n    ----------\n    theta : array_like\n        Parameters for logistic regression. A vecotor of shape (n+1, ).\n    \n    X : array_like\n        The data to use for computing predictions. The rows is the number \n        of points to compute predictions, and columns is the number of\n        features.\n\n    Returns\n    -------\n    p : array_like\n        Predictions and 0 or 1 for each row in X. \n    \n    Instructions\n    ------------\n    Complete the following code to make predictions using your learned \n    logistic regression parameters.You should set p to a vector of 0's and 1's    \n    \"\"\"\n    m = X.shape[0] # Number of training examples\n\n    # You need to return the following variables correctly\n    p = np.zeros(m)\n\n    # ====================== YOUR CODE HERE ======================\n    p = np.round(sigmoid(X.dot(theta.T)))\n\n    \n    # ============================================================\n    return p","execution_count":45,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After you have completed the code in `predict`, we proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normal training without Degree\n\np = predict(thata1, X_test[:,0:3])\nprint('Train Accuracy: {:.2f} %'.format(np.mean(p == y_Newtest) * 100))","execution_count":47,"outputs":[{"output_type":"stream","text":"Train Accuracy: 85.00 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for training using Degree\np = predict(thata2, X_Newtest[:,0:3])\nprint('Train Accuracy: {:.2f} %'.format(np.mean(p == y_Newtest) * 100))","execution_count":51,"outputs":[{"output_type":"stream","text":"Train Accuracy: 80.00 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"testArray = (p == y_test)\ncount = np.array(testArray, dtype=np.bool)\nprint(np.count_nonzero(count), len(testArray))\nper = np.count_nonzero(count)/len(testArray)\nprint(per*100)","execution_count":58,"outputs":[{"output_type":"stream","text":"16 20\n80.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2 Regularized logistic regression\n\nIn this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\nSuppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n\nFirst, we load the data from a CSV file:"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section5\"></a>\n### 2.3 Cost function and gradient\n\nNow you will implement code to compute the cost function and gradient for regularized logistic regression. Complete the code for the function `costFunctionReg` below to return the cost and gradient.\n\nRecall that the regularized cost function in logistic regression is\n\n$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)}\\log \\left( h_\\theta \\left(x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n\nNote that you should not regularize the parameters $\\theta_0$. The gradient of the cost function is a vector where the $j^{th}$ element is defined as follows:\n\n$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\qquad \\text{for } j =0 $$\n\n$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\qquad \\text{for } j \\ge 1 $$\n<a id=\"costFunctionReg\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def costFunctionReg(theta, X, y, lambda_):\n    \"\"\"\n    Compute cost and gradient for logistic regression with regularization.\n    \n    Parameters\n    ----------\n    theta : array_like\n        Logistic regression parameters. A vector with shape (n, ). n is \n        the number of features including any intercept. If we have mapped\n        our initial features into polynomial features, then n is the total \n        number of polynomial features. \n    \n    X : array_like\n        The data set with shape (m x n). m is the number of examples, and\n        n is the number of features (after feature mapping).\n    \n    y : array_like\n        The data labels. A vector with shape (m, ).\n    \n    lambda_ : float\n        The regularization parameter. \n    \n    Returns\n    -------\n    J : float\n        The computed value for the regularized cost function. \n    \n    grad : array_like\n        A vector of shape (n, ) which is the gradient of the cost\n        function with respect to theta, at the current values of theta.\n    \n    Instructions\n    ------------\n    Compute the cost `J` of a particular choice of theta.\n    Compute the partial derivatives and set `grad` to the partial\n    derivatives of the cost w.r.t. each parameter in theta.\n    \"\"\"\n    # Initialize some useful values\n    m = y.size  # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n    grad = np.zeros(theta.shape)\n\n    # ===================== YOUR CODE HERE ======================\n    h = sigmoid(X.dot(theta.T))\n    \n    temp = theta\n    temp[0] = 0\n    \n    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))\n    \n    grad = (1 / m) * (h - y).dot(X) \n    grad = grad + (lambda_ / m) * temp\n    # =============================================================\n    return J, grad","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeCostReg(theta,X,y,lambda_):\n    # Initialize some useful values\n    m = y.size  # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n\n    h = sigmoid(X.dot(theta.T))\n    \n    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(theta))\n    \n    return J","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nLambda = [0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.024]\noptions= {'maxiter': 200}\nfor i in range(len(Lambda)):\n    initTheta = np.zeros(4)\n    res = optimize.minimize(costFunctionReg,\n                            initTheta,\n                            (X_train[:,0:4], y_train, Lambda[i]),\n                            jac=True,\n                            method='TNC',\n                            options=options)\n    print('cost is equal = ',computeCostReg(res.x,X_cross[:,0:4],y_cross,Lambda[i]))\n\n\n\n\n\n","execution_count":61,"outputs":[{"output_type":"stream","text":"cost is equal =  0.35179856440909946\ncost is equal =  0.35185393693382194\ncost is equal =  0.351912182433913\ncost is equal =  0.35202446024880296\ncost is equal =  0.35225358437417137\ncost is equal =  0.35270768706556194\ncost is equal =  0.35362583544731935\ncost is equal =  0.3554878582230348\ncost is equal =  0.3577961444250099\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}